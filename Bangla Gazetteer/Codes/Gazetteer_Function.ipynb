{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yMmUdwp3JaS",
        "outputId": "d20aa9bf-6190-4089-8965-1906e3c590c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/csebuetnlp/normalizer\n",
            "  Cloning https://github.com/csebuetnlp/normalizer to /tmp/pip-req-build-w__hc9dt\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/csebuetnlp/normalizer /tmp/pip-req-build-w__hc9dt\n",
            "  Resolved https://github.com/csebuetnlp/normalizer to commit d405944dde5ceeacb7c2fd3245ae2a9dea5f35c9\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from normalizer==0.0.1) (2023.6.3)\n",
            "Collecting emoji==1.4.2 (from normalizer==0.0.1)\n",
            "  Downloading emoji-1.4.2.tar.gz (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m185.0/185.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy==6.0.3 (from normalizer==0.0.1)\n",
            "  Downloading ftfy-6.0.3.tar.gz (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy==6.0.3->normalizer==0.0.1) (0.2.13)\n",
            "Building wheels for collected packages: normalizer, emoji, ftfy\n",
            "  Building wheel for normalizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for normalizer: filename=normalizer-0.0.1-py3-none-any.whl size=6859 sha256=a4d0b11dc4f401ca9451233facbe5e9ef085bd0a10e47cff2078ea7a2f4c630c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-88l09q_8/wheels/2e/79/9c/cd96d490298305d51d2da11484bb2c25fd1f759a6906708282\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.4.2-py3-none-any.whl size=186460 sha256=c1888d40f42206d3d45204ee0b542eda33c212b9f0d04bfef014e98129208a07\n",
            "  Stored in directory: /root/.cache/pip/wheels/10/f0/fd/4813b1177405693e8da9cdea839f0fb64fde161380e058c827\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-6.0.3-py3-none-any.whl size=41929 sha256=52abb13e73a0cbadc668136522fbb2b8d50945260293ae309caa77623102f219\n",
            "  Stored in directory: /root/.cache/pip/wheels/92/8e/16/c1e4d4d65685d71085e4e27b44d6ed880b0559474c9ee4ff66\n",
            "Successfully built normalizer emoji ftfy\n",
            "Installing collected packages: emoji, ftfy, normalizer\n",
            "Successfully installed emoji-1.4.2 ftfy-6.0.3 normalizer-0.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install tokenizers -q\n",
        "!pip install git+https://github.com/csebuetnlp/normalizer\n",
        "!pip install transformers -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "toxbp60C3KAs"
      },
      "outputs": [],
      "source": [
        "from normalizer import normalize\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import pickle\n",
        "import torch\n",
        "from transformers import ElectraTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPMnsPdW3bpC"
      },
      "outputs": [],
      "source": [
        "\n",
        "model_name = '/content/drive/MyDrive/Thesis/BERTOUTPUT/checkpoint-11000/'\n",
        "tokenizer = ElectraTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YYmonYL3Yuf",
        "outputId": "62587fb2-dead-4985-c7b7-72700c4e2467"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] J\n",
            "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] o\n",
            "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] h\n",
            "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] n\n",
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
            "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0] ট\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0] ো\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0] ক\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0] ি\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0] ও\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]  \n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0] ব\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0] ি\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0] শ\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0] ্\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0] ব\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0] ব\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0] ি\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0] দ\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0] ্\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0] য\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0] া\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0] ল\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0] য\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0] ়\n",
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] এ\n",
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] প\n",
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] ড়\n",
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] ে\n",
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] a\n",
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] n\n",
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] d\n",
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] o\n",
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] w\n",
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] n\n",
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] s\n",
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] a\n",
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] n\n",
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0] i\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1] P\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1] h\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1] o\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1] n\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1] e\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]  \n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1] 1\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1] 4\n",
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] m\n",
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] a\n",
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] d\n",
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] e\n",
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] b\n",
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] y\n",
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0] A\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1] p\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1] p\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1] l\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1] e\n",
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] ,\n",
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] n\n",
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] o\n",
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] t\n",
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] M\n",
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] i\n",
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] c\n",
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] r\n",
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] o\n",
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] s\n",
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] o\n",
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] f\n",
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] t\n",
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] .\n",
            "John [0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            "টোকিও [0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
            "বিশ্ববিদ্যালয় [0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
            "এ [1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "পড়ে [1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "and [1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "owns [1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "an [1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "iPhone [0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
            "14 [0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            "made [1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "by [1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Apple, [0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
            "not [1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Microsoft. [1 0 0 0 0 0 0 0 0 0 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "class TrieNode:\n",
        "    def __init__(self):\n",
        "        self.children = {}\n",
        "        self.is_end_of_word = False\n",
        "\n",
        "class Trie:\n",
        "    def __init__(self):\n",
        "        self.root = TrieNode()\n",
        "        self.entity_tags = [\"PER\", \"LOC\", \"CW\", \"CORP\", \"GRP\", \"PROD\"]\n",
        "        self.tag_encoding = {\"O\": 0, \"B-PER\": 1, \"I-PER\": 2, \"B-LOC\": 3, \"I-LOC\": 4,\n",
        "                             \"B-CW\": 5, \"I-CW\": 6, \"B-CORP\": 7, \"I-CORP\": 8,\n",
        "                             \"B-GRP\": 9, \"I-GRP\": 10, \"B-PROD\": 11, \"I-PROD\": 12}\n",
        "\n",
        "    def insert(self, word, entity_type):\n",
        "        node = self.root\n",
        "        for char in word:\n",
        "            if char not in node.children:\n",
        "                node.children[char] = TrieNode()\n",
        "            node = node.children[char]\n",
        "        node.is_end_of_word = True\n",
        "        node.entity_type = entity_type\n",
        "\n",
        "    def search(self, sentence):\n",
        "        encoding = [0] * len(sentence)\n",
        "        for i in range(len(sentence)):\n",
        "            node = self.root\n",
        "            for j in range(i, len(sentence)):\n",
        "                char = sentence[j]\n",
        "                if char not in node.children:\n",
        "                    break\n",
        "                node = node.children[char]\n",
        "                if node.is_end_of_word:\n",
        "                    entity_type = node.entity_type\n",
        "                    entity_length = j - i + 1\n",
        "                    start_index = i\n",
        "                    end_index = j\n",
        "                    for k in range(start_index, end_index + 1):\n",
        "                        if k == start_index:\n",
        "                            encoding[k] = self.tag_encoding[\"B-\" + entity_type]\n",
        "                        else:\n",
        "                            encoding[k] = self.tag_encoding[\"I-\" + entity_type]\n",
        "                    break\n",
        "        one_hot_encoding = [[0] * 13 for i in range(len(encoding))]\n",
        "        for i, tag in enumerate(encoding):\n",
        "            one_hot_encoding[i][tag] = 1\n",
        "        return one_hot_encoding\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def group_encodings_by_word(encoding, sentence):\n",
        "    # Create an empty list to store the word encodings\n",
        "    word_encodings = []\n",
        "\n",
        "    # Create an empty list to store the current word encoding\n",
        "    current_word_encoding = []\n",
        "\n",
        "    # Create an empty string to store the current word\n",
        "    current_word = \"\"\n",
        "\n",
        "    # Iterate over each character encoding and character in the input encoding list and sentence, respectively\n",
        "    for char_encoding, char in zip(encoding, sentence):\n",
        "        # If the current character is a whitespace character, finish the current word and add its first character encoding to the word encodings list\n",
        "        if char == \" \":\n",
        "            if len(current_word_encoding) > 0:\n",
        "                word_encodings.append(np.array(current_word_encoding[0]))\n",
        "                current_word_encoding = []\n",
        "            current_word = \"\"\n",
        "        # If the current character is part of a word, append the character encoding to the current word encoding and the character to the current word\n",
        "        else:\n",
        "            current_word_encoding.append(char_encoding)\n",
        "            current_word += char\n",
        "\n",
        "    # Add the last word encoding to the word encodings list, if it exists\n",
        "    if len(current_word_encoding) > 0:\n",
        "        word_encodings.append(np.array(current_word_encoding[0]))\n",
        "\n",
        "    # Return the word encodings as a NumPy array\n",
        "    return np.array(word_encodings)\n",
        "\n",
        "import pickle\n",
        "\n",
        "def save_trie(trie, filename):\n",
        "    with open(filename, \"wb\") as f:\n",
        "        pickle.dump(trie, f)\n",
        "\n",
        "def load_trie(filename):\n",
        "    with open(filename, \"rb\") as f:\n",
        "        trie = pickle.load(f)\n",
        "    return trie\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    trie = Trie()\n",
        "\n",
        "    # Insert some named entities into the Trie\n",
        "    trie.insert(\"John\", \"PER\")\n",
        "    trie.insert(\"New York\", \"LOC\")\n",
        "    trie.insert(\"iPhone 14\", \"PROD\")\n",
        "    trie.insert(\"টোকিও বিশ্ববিদ্যালয়\", \"CORP\")\n",
        "    trie.insert(\"Apple\", \"CORP\")\n",
        "    trie.insert(\"Apple\", \"PROD\")\n",
        "    # Search for named entities in a sentence\n",
        "    sentence = \"John টোকিও বিশ্ববিদ্যালয় এ পড়ে and owns an iPhone 14 made by Apple, not Microsoft.\"\n",
        "    encoding = trie.search(sentence)\n",
        "    for i, word in enumerate(sentence):\n",
        "        print(encoding[i],word)\n",
        "\n",
        "    word_encodings = group_encodings_by_word(encoding,sentence)\n",
        "\n",
        "# Print the word encodings\n",
        "    for word, word_encoding in zip(sentence.split(), word_encodings):\n",
        "        print(word, word_encoding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lzi6x6M_4BR5"
      },
      "outputs": [],
      "source": [
        "trie = load_trie('/content/drive/MyDrive/Bracu/THESIS/Trie DS/Trie old.bin')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85oUKuLv3bJK"
      },
      "outputs": [],
      "source": [
        "\n",
        "def Gtoken(text):\n",
        "  inputs = tokenizer.encode_plus(text, return_tensors='pt')\n",
        "\n",
        "  encoded_dict = tokenizer.encode_plus(\n",
        "                  text,       # Sentence to encode.\n",
        "                  add_special_tokens = False, # Add '[CLS]' and '[SEP]'\n",
        "                  max_length = 314,           # Pad & truncate all sentences.\n",
        "                  padding = 'max_length',\n",
        "                  return_attention_mask = True,   # Construct attn. masks.\n",
        "                  return_tensors = 'pt',\n",
        "                  truncation=False)\n",
        "  input_ids = encoded_dict['input_ids']\n",
        "  tokenized = tokenizer.convert_ids_to_tokens([i.item() for i in input_ids.squeeze() if i > 1])\n",
        "  return \"< \"+\" \".join(tokenized)+\" >\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HhjOZaafkga",
        "outputId": "0a86ea02-5ae4-412a-9b3e-19609d4df90d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8XIP7fPWnUt",
        "outputId": "b084cc4b-916d-4839-b419-31f83fa541df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PER\n",
            "LOC\n",
            "CW\n",
            "CORP\n",
            "GRP\n",
            "PROD\n"
          ]
        }
      ],
      "source": [
        "entity_tags = [\"PER\", \"LOC\", \"CW\", \"CORP\", \"GRP\", \"PROD\"]\n",
        "for entity in entity_tags:\n",
        "    with open('{}.txt'.format(entity), 'r', encoding = 'utf-8') as file:\n",
        "        print(entity)\n",
        "        for line in file:\n",
        "            trie.insert(Gtoken(normalize(line.strip())), entity)\n",
        "\n",
        "# save_trie(tree, \"Trie.bin\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Bracu/THESIS/Trie DS/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLb-2b-FcJE4",
        "outputId": "74cfe26a-9ea2-48db-ae80-794739363912"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Bracu/THESIS/Trie DS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jV-tDmw3wD-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05c4e614-1981-4406-b3b6-69ca366dabcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PER\n",
            "PER\n",
            "LOC\n",
            "LOC\n",
            "CW\n",
            "CW\n",
            "CORP\n",
            "CORP\n",
            "GRP\n",
            "GRP\n",
            "PROD\n",
            "PROD\n"
          ]
        }
      ],
      "source": [
        "\n",
        "tree = Trie()\n",
        "entity_tags = [\"PER\", \"LOC\", \"CW\", \"CORP\", \"GRP\", \"PROD\"]\n",
        "for entity in entity_tags:\n",
        "    with open('KE/{}/{}.txt'.format(entity,entity), 'r', encoding = 'utf-8') as file:\n",
        "        print(entity)\n",
        "        for line in file:\n",
        "            tree.insert(Gtoken(normalize(line.strip())), entity)\n",
        "    with open('KE/{}.txt'.format(entity), 'r', encoding = 'utf-8') as file:\n",
        "        print(entity)\n",
        "        for line in file:\n",
        "            tree.insert(Gtoken(normalize(line.strip())), entity)\n",
        "\n",
        "save_trie(tree, \"/content/Trie.bin\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cezUEVTz4bVU",
        "outputId": "922e54fe-b680-4e6d-baa2-33c15bb02383"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "< [1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "আন্দ্রে [0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
            "##য়া [1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "নাভ [1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "##াগের [1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "##ো [1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "এবং [1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "অ্যা [1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "##গো [1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "##স্টি [1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "##নো [1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "বেজ [1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "##ানোর [1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "প্রতিকৃতি [0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
            "বাড়ীর [1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "ছোট [0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
            "বউ [0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
            "অ্যাম [0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
            "##িটি [0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
            "ইনস্টিটিউট [0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
            "অফ [1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "বায়ো [1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "##টেক [1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "##নো [1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "##লজি [1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "> [1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "(26, 13)\n"
          ]
        }
      ],
      "source": [
        "sentence = Gtoken(normalize(\"আন্দ্রেয়া নাভাগেরো এবং অ্যাগোস্টিনো বেজানোর প্রতিকৃতি বাড়ীর ছোট বউ অ্যামিটি ইনস্টিটিউট অফ বায়োটেকনোলজি\"))\n",
        "encoding = trie.search(sentence)\n",
        "trie.insert(Gtoken(normalize('কুত্তা')), 'PER')\n",
        "# for i, word in enumerate(sentence):\n",
        "#     # print(encoding[i],word)\n",
        "\n",
        "word_encodings = group_encodings_by_word(encoding,sentence)\n",
        "\n",
        "# Print the word encodings\n",
        "for word, word_encoding in zip(sentence.split(), word_encodings):\n",
        "    print(word, word_encoding)\n",
        "\n",
        "print(word_encodings.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sapBdDs4v0g"
      },
      "outputs": [],
      "source": [
        "def Gazetteer(sentence, pad = True, dtype ='torch'):\n",
        "  sentence = Gtoken(normalize(sentence))\n",
        "  encoding = trie.search(sentence)\n",
        "  tensor = group_encodings_by_word(encoding,sentence)\n",
        "  if pad:\n",
        "    current_size = tensor.size(0)\n",
        "    if current_size >= 64:\n",
        "        return tensor[:64, :]\n",
        "\n",
        "    padded_tensor = torch.full((64, 13),-100.)\n",
        "    padded_tensor[:current_size, :] = tensor\n",
        "    if dtype == 'numpy':\n",
        "        return padded_tensor.numpy()\n",
        "    elif dtype == 'tensorflow':\n",
        "        import tensorflow as tf\n",
        "        return tf.convert_to_tensor(padded_tensor)\n",
        "    else:\n",
        "      return padded_tensor\n",
        "  else:\n",
        "    if dtype == 'numpy':\n",
        "        return padded_tensor.numpy()\n",
        "    elif dtype == 'tensorflow':\n",
        "        import tensorflow as tf\n",
        "        return tf.convert_to_tensor(tensor)\n",
        "    else:\n",
        "        return tensor"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DR-RtAAkiXep"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}