{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from bnlp import POS\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import sklearn\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn_crfsuite as crfsuite\n",
    "from sklearn_crfsuite import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    with open(filename, 'r',encoding='utf-8') as f:\n",
    "        data = [line.strip().split(' _ _ ') for line in f.readlines()]\n",
    "        # return data\n",
    "        # empty lines are used to separate sentences\n",
    "        # separate them into sentences\n",
    "        sentences = []\n",
    "        cur = []\n",
    "        for line in data:\n",
    "            if line == ['']:\n",
    "                cur = [tuple(line) for line in cur]\n",
    "                sentences.append(cur)\n",
    "                cur = []\n",
    "            else:\n",
    "                # for p in punctuations:\n",
    "                #     line[0] = line[0].replace(p, '')\n",
    "                if len(line[0]) == 0:\n",
    "                    line[0] = ' '\n",
    "                cur.append(line)\n",
    "        # # convert each list to a tuple\n",
    "        sentences.append(cur)\n",
    "        return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = load_data('train.txt')\n",
    "dev = load_data('dev.txt')\n",
    "test = load_data('test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'bnlp' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/sagorbrur/bnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn_pos = POS()\n",
    "model_path = \"bnlp/model/bn_pos.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_pos(sentence):\n",
    "    # first one of the tuple is the word and the second one is the ner  \n",
    "    words = []\n",
    "    for word in sentence:\n",
    "        words.append(word[0])\n",
    "    pos = bn_pos.tag(model_path, words)\n",
    "    ## add ner back to the tuple\n",
    "    ret = []\n",
    "    for i in range(len(pos)):\n",
    "        word = sentence[i][0]\n",
    "        ner = sentence[i][1]\n",
    "        ret.append((word, pos[i][1], ner))\n",
    "    return ret\n",
    "\n",
    "def add_pos_to_all(sents):\n",
    "    ret = []\n",
    "    for i in tqdm(range(len(sents))):\n",
    "        # if i % 1000 == 0:\n",
    "        #     print(i)\n",
    "        ret.append(add_pos(sents[i]))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15301/15301 [02:12<00:00, 115.80it/s]\n"
     ]
    }
   ],
   "source": [
    "train_sents =  add_pos_to_all(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('আরাকি', 'NP', 'O'),\n",
       " ('বলেছিলেন', 'VM', 'O'),\n",
       " ('যে', 'CSB', 'O'),\n",
       " ('সোনালী', 'JJ', 'B-CW'),\n",
       " ('বাতাস', 'NC', 'I-CW'),\n",
       " ('এর', 'PPR', 'O'),\n",
       " ('মূল', 'JJ', 'O'),\n",
       " ('লক্ষ্য', 'NC', 'O'),\n",
       " ('ছিল', 'VM', 'O'),\n",
       " ('সুন্দর', 'JJ', 'O'),\n",
       " ('পুরুষদের', 'NC', 'O'),\n",
       " ('আঁকা,', 'NC', 'O'),\n",
       " ('যারা', 'PRL', 'O'),\n",
       " ('কেবল', 'AMN', 'O'),\n",
       " ('এমন', 'DAB', 'O'),\n",
       " ('একটি', 'JQ', 'O'),\n",
       " ('পৃথিবীতে', 'NC', 'O'),\n",
       " ('থাকতে', 'VM', 'O'),\n",
       " ('পারে', 'VAUX', 'O'),\n",
       " ('যেখানে', 'ALC', 'O'),\n",
       " ('একজনের', 'NC', 'O'),\n",
       " ('শাস্তি', 'NC', 'O'),\n",
       " ('পূরণের', 'NC', 'O'),\n",
       " ('সৌন্দর্য', 'NC', 'O'),\n",
       " ('আছে।', 'PU', 'O')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sents[69]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 801/801 [00:07<00:00, 114.21it/s]\n"
     ]
    }
   ],
   "source": [
    "dev_sents = add_pos_to_all(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('২০০৯', 'RDF', 'O'),\n",
       " ('সালে,', 'NC', 'O'),\n",
       " ('রেকর্ডিং', 'NC', 'O'),\n",
       " ('লাইব্রেরি', 'NC', 'B-GRP'),\n",
       " ('অফ', 'NC', 'I-GRP'),\n",
       " ('কংগ্রেস', 'NP', 'I-GRP'),\n",
       " ('সংরক্ষণের', 'NC', 'O'),\n",
       " ('জন্য', 'PP', 'O'),\n",
       " ('নির্বাচিত', 'JJ', 'O'),\n",
       " ('হয়েছিল।', 'PU', 'O')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_sents[69]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 133120/133120 [19:20<00:00, 114.68it/s]\n"
     ]
    }
   ],
   "source": [
    "test_sents = add_pos_to_all(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('পেটিকোট', 'NC', 'B-PROD'),\n",
       " ('এর', 'PPR', 'O'),\n",
       " ('জন্য', 'PP', 'O'),\n",
       " ('সঠিক', 'JJ', 'O'),\n",
       " ('তাপমাত্রা', 'NC', 'O')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sents[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_sents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39msaved_pickles/train_sents.pickle\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m handle:\n\u001b[1;32m----> 2\u001b[0m     pickle\u001b[39m.\u001b[39mdump(train_sents, handle, protocol\u001b[39m=\u001b[39mpickle\u001b[39m.\u001b[39mHIGHEST_PROTOCOL)\n\u001b[0;32m      4\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39msaved_pickles/dev_sents.pickle\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m handle:\n\u001b[0;32m      5\u001b[0m     pickle\u001b[39m.\u001b[39mdump(train_sents, handle, protocol\u001b[39m=\u001b[39mpickle\u001b[39m.\u001b[39mHIGHEST_PROTOCOL)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_sents' is not defined"
     ]
    }
   ],
   "source": [
    "# with open('saved_pickles/train_sents.pickle', 'wb') as handle:\n",
    "#     pickle.dump(train_sents, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open('saved_pickles/dev_sents.pickle', 'wb') as handle:\n",
    "#     pickle.dump(dev_sents, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open('saved_pickles/test_sents.pickle', 'wb') as handle:\n",
    "#     pickle.dump(test_sents, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word frequency information\n",
    "\n",
    "def get_all_words(sents):\n",
    "    words = []\n",
    "    for i in range(len(sents)):\n",
    "        for w in sents[i]:\n",
    "            words += [w[0]]\n",
    "    return words\n",
    "\n",
    "all_train_words = get_all_words(train_sents)\n",
    "len(all_train_words)\n",
    "\n",
    "word_freq = {}\n",
    "for w in all_train_words:\n",
    "    if w in word_freq:\n",
    "        word_freq[w] += 1\n",
    "    else:\n",
    "        word_freq[w] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gazzeter Implementation\n",
    "\n",
    "GRP_bn = {}\n",
    "LOC_bn = {}\n",
    "PER_bn = {}\n",
    "PROD_bn = {}\n",
    "CORP_bn = {}\n",
    "CW_bn = {}\n",
    "\n",
    "\n",
    "with open('gazz_main/CORP.txt') as file:\n",
    "    lines = [x.strip() for x in file.readlines()]\n",
    "    for l in lines:\n",
    "        CORP_bn[l] = 1\n",
    "        words = l.split()\n",
    "        for size in range(2, 4, 1):\n",
    "            for i in range(len(words)):\n",
    "                if (i + size) >= len(words):\n",
    "                    break\n",
    "                CORP_bn[\" \".join(words[i: i+size])] = 1\n",
    "\n",
    "with open('gazz_main/CW.txt') as file:\n",
    "    lines = [x.strip() for x in file.readlines()]\n",
    "    for l in lines:\n",
    "        CW_bn[l] = 1\n",
    "        words = l.split()\n",
    "        for size in range(2, 4, 1):\n",
    "            for i in range(len(words)):\n",
    "                if (i + size) >= len(words):\n",
    "                    break\n",
    "                CW_bn[\" \".join(words[i: i+size])] = 1\n",
    "\n",
    "with open('gazz_main/GRP.txt') as file:\n",
    "    lines = [x.strip() for x in file.readlines()]\n",
    "    for l in lines:\n",
    "        GRP_bn[l] = 1\n",
    "        words = l.split()\n",
    "        for size in range(2, 4, 1):\n",
    "            for i in range(len(words)):\n",
    "                if (i + size) >= len(words):\n",
    "                    break\n",
    "                GRP_bn[\" \".join(words[i: i+size])] = 1\n",
    "\n",
    "\n",
    "with open('gazz_main/LOC.txt') as file:\n",
    "    lines = [x.strip() for x in file.readlines()]\n",
    "    for l in lines:\n",
    "        LOC_bn[l] = 1\n",
    "        words = l.split()\n",
    "        for size in range(2, 4, 1):\n",
    "            for i in range(len(words)):\n",
    "                if (i + size) >= len(words):\n",
    "                    break\n",
    "                LOC_bn[\" \".join(words[i: i+size])] = 1\n",
    "\n",
    "with open('gazz_main/PER.txt') as file:\n",
    "    lines = [x.strip() for x in file.readlines()]\n",
    "    for l in lines:\n",
    "        PER_bn[l] = 1\n",
    "        words = l.split()\n",
    "        for size in range(2, 4, 1):\n",
    "            for i in range(len(words)):\n",
    "                if (i + size) >= len(words):\n",
    "                    break\n",
    "                PER_bn[\" \".join(words[i: i+size])] = 1\n",
    "\n",
    "\n",
    "with open('gazz_main/PROD.txt') as file:\n",
    "    lines = [x.strip() for x in file.readlines()]\n",
    "    for l in lines:\n",
    "        PROD_bn[l] = 1\n",
    "        words = l.split()\n",
    "        for size in range(2, 4, 1):\n",
    "            for i in range(len(words)):\n",
    "                if (i + size) >= len(words):\n",
    "                    break\n",
    "                PROD_bn[\" \".join(words[i: i+size])] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature extraction for Conditional Random Field - Bangla NER\n",
    "\n",
    "def wordToFeatures(sent, idx):\n",
    "    word = sent[idx][0]\n",
    "    postag = sent[idx][1]\n",
    "    \n",
    "    # cluster_id = kmeans.predict([w2v.get_vector(word)])[0]\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word': word,\n",
    "        # 'cluster_id': cluster_id,\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word[:3]': word[:3],\n",
    "        'word[:2]': word[:2],\n",
    "        'word.isdigit': word.isdigit(),\n",
    "        'index': idx,\n",
    "        'length': len(word),\n",
    "        'postag': postag,\n",
    "        'freq': word_freq[word] if word in word_freq else 0,\n",
    "        'is_corp': int(word in CORP_bn), \n",
    "        'is_cw': int(word in CW_bn), \n",
    "        'is_grp': int(word in GRP_bn), \n",
    "        'is_loc': int(word in LOC_bn), \n",
    "        'is_per': int(word in PER_bn), \n",
    "        'is_prod': int(word in PROD_bn),\n",
    "    }\n",
    "    \n",
    "    if idx > 0:\n",
    "        sub = sent[idx-1][0] + \" \"+ sent[idx][0]\n",
    "        if sub in CORP_bn:\n",
    "            features['is_corp'] = 2\n",
    "        if sub in CW_bn:\n",
    "            features['is_cw'] = 2\n",
    "        if sub in GRP_bn:\n",
    "            features['is_grp'] = 2\n",
    "        if sub in LOC_bn:\n",
    "            features['is_loc'] = 2\n",
    "        if sub in PER_bn:\n",
    "            features['is_per'] = 2\n",
    "        if sub in PROD_bn:\n",
    "            features['is_prod'] = 2\n",
    "    \n",
    "    if idx < len(sent) - 1:\n",
    "        sub = sent[idx][0] + \" \"+ sent[idx+1][0]\n",
    "        if sub in CORP_bn:\n",
    "            features['is_corp'] = 2\n",
    "        if sub in CW_bn:\n",
    "            features['is_cw'] = 2\n",
    "        if sub in GRP_bn:\n",
    "            features['is_grp'] = 2\n",
    "        if sub in LOC_bn:\n",
    "            features['is_loc'] = 2\n",
    "        if sub in PER_bn:\n",
    "            features['is_per'] = 2\n",
    "        if sub in PROD_bn:\n",
    "            features['is_prod'] = 2\n",
    "    \n",
    "    if idx > 1:\n",
    "        sub = sent[idx-2][0] + \" \"+ sent[idx-1][0] + \" \" + sent[idx][0]\n",
    "        if sub in CORP_bn:\n",
    "            features['is_corp'] = 2\n",
    "        if sub in CW_bn:\n",
    "            features['is_cw'] = 2\n",
    "        if sub in GRP_bn:\n",
    "            features['is_grp'] = 2\n",
    "        if sub in LOC_bn:\n",
    "            features['is_loc'] = 2\n",
    "        if sub in PER_bn:\n",
    "            features['is_per'] = 2\n",
    "        if sub in PROD_bn:\n",
    "            features['is_prod'] = 2\n",
    "    if idx < len(sent) - 2:\n",
    "        sub = sent[idx][0] + \" \"+ sent[idx+1][0] + \" \" + sent[idx+2][0]\n",
    "        if sub in CORP_bn:\n",
    "            features['is_corp'] = 2\n",
    "        if sub in CW_bn:\n",
    "            features['is_cw'] = 2\n",
    "        if sub in GRP_bn:\n",
    "            features['is_grp'] = 2\n",
    "        if sub in LOC_bn:\n",
    "            features['is_loc'] = 2\n",
    "        if sub in PER_bn:\n",
    "            features['is_per'] = 2\n",
    "        if sub in PROD_bn:\n",
    "            features['is_prod'] = 2\n",
    "        \n",
    "    \n",
    "    for i in range(1, 3):\n",
    "        if idx < i:\n",
    "            break\n",
    "        wordi = sent[idx-i][0]\n",
    "        postagi = sent[idx-i][1]\n",
    "        # cluster_id_i = kmeans.predict([w2v.get_vector(wordi)])[0]\n",
    "        features.update({\n",
    "            '-{}:word'.format(i): wordi,\n",
    "            # '-{}:cluster_id'.format(i): cluster_id_i,\n",
    "            '-{}:word[-3:]'.format(i): wordi[-3:],\n",
    "            '-{}:word[-2:]'.format(i): wordi[-2:],\n",
    "            '-{}:word[:3]'.format(i): wordi[:3],\n",
    "            '-{}:word[:2]'.format(i): wordi[:2],\n",
    "            '-{}:word.isdigit'.format(i): wordi.isdigit(),\n",
    "            '-{}:postag'.format(i): postagi,\n",
    "            '-{}:is_corp'.format(i): int(word in CORP_bn), \n",
    "            '-{}:is_cw'.format(i): int(word in CW_bn), \n",
    "            '-{}:is_grp'.format(i): int(word in GRP_bn), \n",
    "            '-{}:is_loc'.format(i): int(word in LOC_bn), \n",
    "            '-{}:is_per'.format(i): int(word in PER_bn), \n",
    "            '-{}:is_prod'.format(i): int(word in PROD_bn),\n",
    "        })\n",
    "    \n",
    "    for i in range(1, 3):\n",
    "        if (idx+i) >= len(sent):\n",
    "            break\n",
    "        wordi = sent[idx+i][0]\n",
    "        postagi = sent[idx+i][1]\n",
    "        # cluster_id_i = kmeans.predict([w2v.get_vector(wordi)])[0]\n",
    "        features.update({\n",
    "            '{}:word'.format(i): wordi,\n",
    "            # '{}:cluster_id'.format(i): cluster_id_i,\n",
    "            '{}:word[-3:]'.format(i): wordi[-3:],\n",
    "            '{}:word[-2:]'.format(i): wordi[-2:],\n",
    "            '{}:word[:3]'.format(i): wordi[:3],\n",
    "            '{}:word[:2]'.format(i): wordi[:2],\n",
    "            '{}:word.isdigit'.format(i): wordi.isdigit(),\n",
    "            '{}:postag'.format(i): postagi,\n",
    "            '-{}:is_corp'.format(i): int(word in CORP_bn), \n",
    "            '-{}:is_cw'.format(i): int(word in CW_bn), \n",
    "            '-{}:is_grp'.format(i): int(word in GRP_bn), \n",
    "            '-{}:is_loc'.format(i): int(word in LOC_bn), \n",
    "            '-{}:is_per'.format(i): int(word in PER_bn), \n",
    "            '-{}:is_prod'.format(i): int(word in PROD_bn),\n",
    "        })\n",
    "        \n",
    "    if idx == 0:\n",
    "        features['BOS'] = True\n",
    "    if idx == len(sent) - 1:\n",
    "        features['EOS'] = True\n",
    "    \n",
    "    return features\n",
    "\n",
    "def sentTofeatures(sent):\n",
    "    return [wordToFeatures(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sentTolabels(sent):\n",
    "    return [label for token, postag, label in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newwwwwwwwwwwwwwwwwwwwwwwww\n",
    "\n",
    "def wordToFeatures(sent, idx):\n",
    "    word = sent[idx][0]\n",
    "    postag = sent[idx][1]\n",
    "    \n",
    "    # cluster_id = kmeans.predict([w2v.get_vector(word)])[0]\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word': word,\n",
    "        # 'cluster_id': cluster_id,\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word[:3]': word[:3],\n",
    "        'word[:2]': word[:2],\n",
    "        'word.isdigit': word.isdigit(),\n",
    "        'index': idx,\n",
    "        'length': len(word),\n",
    "        'postag': postag,\n",
    "        'freq': word_freq[word] if word in word_freq else 0,\n",
    "        'is_corp': int(word in CORP_bn), \n",
    "        'is_cw': int(word in CW_bn), \n",
    "        'is_grp': int(word in GRP_bn), \n",
    "        'is_loc': int(word in LOC_bn), \n",
    "        'is_per': int(word in PER_bn), \n",
    "        'is_prod': int(word in PROD_bn),\n",
    "    }\n",
    "    \n",
    "    if idx > 0:\n",
    "        sub = sent[idx-1][0] + \" \"+ sent[idx][0]\n",
    "        if sub in CORP_bn:\n",
    "            features['is_corp'] = 2\n",
    "        if sub in CW_bn:\n",
    "            features['is_cw'] = 2\n",
    "        if sub in GRP_bn:\n",
    "            features['is_grp'] = 2\n",
    "        if sub in LOC_bn:\n",
    "            features['is_loc'] = 2\n",
    "        if sub in PER_bn:\n",
    "            features['is_per'] = 2\n",
    "        if sub in PROD_bn:\n",
    "            features['is_prod'] = 2\n",
    "    \n",
    "    if idx < len(sent) - 1:\n",
    "        sub = sent[idx][0] + \" \"+ sent[idx+1][0]\n",
    "        if sub in CORP_bn:\n",
    "            features['is_corp'] = 2\n",
    "        if sub in CW_bn:\n",
    "            features['is_cw'] = 2\n",
    "        if sub in GRP_bn:\n",
    "            features['is_grp'] = 2\n",
    "        if sub in LOC_bn:\n",
    "            features['is_loc'] = 2\n",
    "        if sub in PER_bn:\n",
    "            features['is_per'] = 2\n",
    "        if sub in PROD_bn:\n",
    "            features['is_prod'] = 2\n",
    "    \n",
    "    if idx > 1:\n",
    "        sub = sent[idx-2][0] + \" \"+ sent[idx-1][0] + \" \" + sent[idx][0]\n",
    "        if sub in CORP_bn:\n",
    "            features['is_corp'] = 2\n",
    "        if sub in CW_bn:\n",
    "            features['is_cw'] = 2\n",
    "        if sub in GRP_bn:\n",
    "            features['is_grp'] = 2\n",
    "        if sub in LOC_bn:\n",
    "            features['is_loc'] = 2\n",
    "        if sub in PER_bn:\n",
    "            features['is_per'] = 2\n",
    "        if sub in PROD_bn:\n",
    "            features['is_prod'] = 2\n",
    "    if idx < len(sent) - 2:\n",
    "        sub = sent[idx][0] + \" \"+ sent[idx+1][0] + \" \" + sent[idx+2][0]\n",
    "        if sub in CORP_bn:\n",
    "            features['is_corp'] = 2\n",
    "        if sub in CW_bn:\n",
    "            features['is_cw'] = 2\n",
    "        if sub in GRP_bn:\n",
    "            features['is_grp'] = 2\n",
    "        if sub in LOC_bn:\n",
    "            features['is_loc'] = 2\n",
    "        if sub in PER_bn:\n",
    "            features['is_per'] = 2\n",
    "        if sub in PROD_bn:\n",
    "            features['is_prod'] = 2\n",
    "        \n",
    "    \n",
    "    for i in range(1, 3):\n",
    "        if idx < i:\n",
    "            break\n",
    "        wordi = sent[idx-i][0]\n",
    "        postagi = sent[idx-i][1]\n",
    "        # cluster_id_i = kmeans.predict([w2v.get_vector(wordi)])[0]\n",
    "        features.update({\n",
    "            '-{}:word'.format(i): wordi,\n",
    "            # '-{}:cluster_id'.format(i): cluster_id_i,\n",
    "            '-{}:word[-3:]'.format(i): wordi[-3:],\n",
    "            '-{}:word[-2:]'.format(i): wordi[-2:],\n",
    "            '-{}:word[:3]'.format(i): wordi[:3],\n",
    "            '-{}:word[:2]'.format(i): wordi[:2],\n",
    "            '-{}:word.isdigit'.format(i): wordi.isdigit(),\n",
    "            '-{}:postag'.format(i): postagi,\n",
    "            '-{}:is_corp'.format(i): int(wordi in CORP_bn), \n",
    "            '-{}:is_cw'.format(i): int(wordi in CW_bn), \n",
    "            '-{}:is_grp'.format(i): int(wordi in GRP_bn), \n",
    "            '-{}:is_loc'.format(i): int(wordi in LOC_bn), \n",
    "            '-{}:is_per'.format(i): int(wordi in PER_bn), \n",
    "            '-{}:is_prod'.format(i): int(wordi in PROD_bn),\n",
    "        })\n",
    "    \n",
    "    for i in range(1, 3):\n",
    "        if (idx+i) >= len(sent):\n",
    "            break\n",
    "        wordi = sent[idx+i][0]\n",
    "        postagi = sent[idx+i][1]\n",
    "        # cluster_id_i = kmeans.predict([w2v.get_vector(wordi)])[0]\n",
    "        features.update({\n",
    "            '{}:word'.format(i): wordi,\n",
    "            # '{}:cluster_id'.format(i): cluster_id_i,\n",
    "            '{}:word[-3:]'.format(i): wordi[-3:],\n",
    "            '{}:word[-2:]'.format(i): wordi[-2:],\n",
    "            '{}:word[:3]'.format(i): wordi[:3],\n",
    "            '{}:word[:2]'.format(i): wordi[:2],\n",
    "            '{}:word.isdigit'.format(i): wordi.isdigit(),\n",
    "            '{}:postag'.format(i): postagi,\n",
    "            '-{}:is_corp'.format(i): int(wordi in CORP_bn), \n",
    "            '-{}:is_cw'.format(i): int(wordi in CW_bn), \n",
    "            '-{}:is_grp'.format(i): int(wordi in GRP_bn), \n",
    "            '-{}:is_loc'.format(i): int(wordi in LOC_bn), \n",
    "            '-{}:is_per'.format(i): int(wordi in PER_bn), \n",
    "            '-{}:is_prod'.format(i): int(wordi in PROD_bn),\n",
    "        })\n",
    "        \n",
    "    if idx == 0:\n",
    "        features['BOS'] = True\n",
    "    if idx == len(sent) - 1:\n",
    "        features['EOS'] = True\n",
    "    \n",
    "    return features\n",
    "\n",
    "def sentTofeatures(sent):\n",
    "    return [wordToFeatures(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sentTolabels(sent):\n",
    "    return [label for token, postag, label in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 4.33 s\n",
      "Wall time: 4.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train = [sentTofeatures(s) for s in train_sents]\n",
    "y_train = [sentTolabels(s) for s in train_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 234 ms\n",
      "Wall time: 240 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_dev = [sentTofeatures(s) for s in dev_sents]\n",
    "y_dev = [sentTolabels(s) for s in dev_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 14.2 s\n",
      "Wall time: 14.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_test = [sentTofeatures(s) for s in test_sents]\n",
    "y_test = [sentTolabels(s) for s in test_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading training data to CRFsuite: 100%|██████████| 15301/15301 [00:05<00:00, 3051.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature generation\n",
      "type: CRF1d\n",
      "feature.minfreq: 0.000000\n",
      "feature.possible_states: 0\n",
      "feature.possible_transitions: 1\n",
      "0....1....2....3....4....5....6....7....8....9....10\n",
      "Number of features: 396086\n",
      "Seconds required: 1.626\n",
      "\n",
      "L-BFGS optimization\n",
      "c1: 0.100000\n",
      "c2: 0.100000\n",
      "num_memories: 6\n",
      "max_iterations: 150\n",
      "epsilon: 0.000010\n",
      "stop: 10\n",
      "delta: 0.000010\n",
      "linesearch: MoreThuente\n",
      "linesearch.max_iterations: 20\n",
      "\n",
      "Iter 1   time=3.24  loss=382243.43 active=389773 feature_norm=0.12\n",
      "Iter 2   time=0.65  loss=379054.96 active=381541 feature_norm=0.12\n",
      "Iter 3   time=0.65  loss=180566.12 active=330603 feature_norm=0.37\n",
      "Iter 4   time=2.59  loss=179367.26 active=370275 feature_norm=0.42\n",
      "Iter 5   time=1.28  loss=169263.38 active=341757 feature_norm=0.29\n",
      "Iter 6   time=0.65  loss=164569.06 active=355790 feature_norm=0.33\n",
      "Iter 7   time=1.32  loss=162782.85 active=367587 feature_norm=0.35\n",
      "Iter 8   time=0.67  loss=162209.30 active=369662 feature_norm=0.35\n",
      "Iter 9   time=1.28  loss=161617.59 active=369933 feature_norm=0.36\n",
      "Iter 10  time=0.64  loss=161481.32 active=370035 feature_norm=0.36\n",
      "Iter 11  time=0.64  loss=161349.45 active=368186 feature_norm=0.36\n",
      "Iter 12  time=0.64  loss=161167.52 active=367789 feature_norm=0.37\n",
      "Iter 13  time=0.65  loss=161034.57 active=367047 feature_norm=0.37\n",
      "Iter 14  time=0.64  loss=160638.51 active=364089 feature_norm=0.38\n",
      "Iter 15  time=0.64  loss=160614.01 active=364226 feature_norm=0.38\n",
      "Iter 16  time=0.64  loss=160272.27 active=365869 feature_norm=0.38\n",
      "Iter 17  time=0.64  loss=160065.93 active=365319 feature_norm=0.38\n",
      "Iter 18  time=0.64  loss=158945.81 active=358454 feature_norm=0.38\n",
      "Iter 19  time=0.63  loss=157974.86 active=359079 feature_norm=0.41\n",
      "Iter 20  time=0.64  loss=156768.66 active=355701 feature_norm=0.44\n",
      "Iter 21  time=0.66  loss=155378.01 active=354685 feature_norm=0.48\n",
      "Iter 22  time=0.65  loss=153048.83 active=352221 feature_norm=0.55\n",
      "Iter 23  time=0.66  loss=149513.48 active=355329 feature_norm=0.76\n",
      "Iter 24  time=0.67  loss=147038.56 active=355743 feature_norm=0.94\n",
      "Iter 25  time=0.67  loss=142267.08 active=361057 feature_norm=1.19\n",
      "Iter 26  time=0.64  loss=137284.11 active=362586 feature_norm=1.41\n",
      "Iter 27  time=2.52  loss=133217.20 active=363532 feature_norm=1.65\n",
      "Iter 28  time=0.64  loss=118402.31 active=361512 feature_norm=2.76\n",
      "Iter 29  time=1.27  loss=107041.16 active=362988 feature_norm=3.78\n",
      "Iter 30  time=0.66  loss=90492.32 active=365332 feature_norm=6.50\n",
      "Iter 31  time=0.64  loss=78688.31 active=364618 feature_norm=7.79\n",
      "Iter 32  time=0.64  loss=70132.55 active=364298 feature_norm=9.41\n",
      "Iter 33  time=1.27  loss=66718.13 active=363669 feature_norm=10.17\n",
      "Iter 34  time=0.64  loss=57949.31 active=361133 feature_norm=13.19\n",
      "Iter 35  time=0.64  loss=54757.20 active=360110 feature_norm=14.71\n",
      "Iter 36  time=0.64  loss=50638.23 active=352908 feature_norm=16.02\n",
      "Iter 37  time=0.64  loss=47461.63 active=351292 feature_norm=17.61\n",
      "Iter 38  time=3.16  loss=45002.85 active=346499 feature_norm=19.11\n",
      "Iter 39  time=0.64  loss=41051.09 active=338275 feature_norm=22.36\n",
      "Iter 40  time=0.63  loss=39308.97 active=339912 feature_norm=24.16\n",
      "Iter 41  time=0.64  loss=36395.78 active=310679 feature_norm=31.85\n",
      "Iter 42  time=1.89  loss=35838.21 active=318321 feature_norm=32.06\n",
      "Iter 43  time=0.66  loss=35153.67 active=317249 feature_norm=32.41\n",
      "Iter 44  time=1.26  loss=34293.73 active=311460 feature_norm=34.31\n",
      "Iter 45  time=1.89  loss=33122.13 active=310622 feature_norm=35.42\n",
      "Iter 46  time=0.64  loss=31748.87 active=314352 feature_norm=36.13\n",
      "Iter 47  time=0.64  loss=30780.45 active=313654 feature_norm=37.22\n",
      "Iter 48  time=1.26  loss=29429.90 active=308421 feature_norm=40.23\n",
      "Iter 49  time=0.65  loss=29065.30 active=310056 feature_norm=39.91\n",
      "Iter 50  time=0.64  loss=28615.69 active=310273 feature_norm=40.16\n",
      "Iter 51  time=0.65  loss=27782.62 active=309862 feature_norm=41.90\n",
      "Iter 52  time=0.64  loss=26850.19 active=309852 feature_norm=43.81\n",
      "Iter 53  time=0.64  loss=25528.08 active=306222 feature_norm=47.77\n",
      "Iter 54  time=0.64  loss=24688.56 active=299593 feature_norm=59.07\n",
      "Iter 55  time=0.64  loss=22638.17 active=296175 feature_norm=65.84\n",
      "Iter 56  time=0.65  loss=21946.90 active=293651 feature_norm=67.23\n",
      "Iter 57  time=0.64  loss=21126.26 active=289712 feature_norm=72.65\n",
      "Iter 58  time=0.64  loss=20547.96 active=293283 feature_norm=73.04\n",
      "Iter 59  time=0.65  loss=20048.62 active=292302 feature_norm=74.63\n",
      "Iter 60  time=3.18  loss=19973.45 active=289922 feature_norm=68.84\n",
      "Iter 61  time=0.67  loss=17804.10 active=288593 feature_norm=85.27\n",
      "Iter 62  time=0.65  loss=17028.82 active=273016 feature_norm=87.95\n",
      "Iter 63  time=0.64  loss=16177.14 active=275456 feature_norm=89.70\n",
      "Iter 64  time=0.64  loss=15369.36 active=274224 feature_norm=93.23\n",
      "Iter 65  time=1.89  loss=15152.47 active=270806 feature_norm=96.85\n",
      "Iter 66  time=0.64  loss=13525.35 active=260908 feature_norm=104.92\n",
      "Iter 67  time=0.64  loss=12759.41 active=258439 feature_norm=108.30\n",
      "Iter 68  time=0.64  loss=11846.32 active=250065 feature_norm=112.01\n",
      "Iter 69  time=1.27  loss=11553.87 active=245379 feature_norm=115.70\n",
      "Iter 70  time=0.64  loss=10508.64 active=242767 feature_norm=119.00\n",
      "Iter 71  time=0.64  loss=10090.85 active=241680 feature_norm=121.16\n",
      "Iter 72  time=0.64  loss=9530.23  active=233583 feature_norm=125.75\n",
      "Iter 73  time=0.65  loss=9349.69  active=228683 feature_norm=126.25\n",
      "Iter 74  time=0.65  loss=8994.10  active=230689 feature_norm=127.01\n",
      "Iter 75  time=0.64  loss=8854.33  active=229700 feature_norm=127.71\n",
      "Iter 76  time=0.64  loss=8458.80  active=222216 feature_norm=128.74\n",
      "Iter 77  time=0.64  loss=8283.05  active=218739 feature_norm=129.35\n",
      "Iter 78  time=0.64  loss=7961.48  active=213951 feature_norm=131.49\n",
      "Iter 79  time=0.64  loss=7842.94  active=210886 feature_norm=131.84\n",
      "Iter 80  time=0.64  loss=7641.18  active=207095 feature_norm=132.40\n",
      "Iter 81  time=0.64  loss=7545.92  active=205594 feature_norm=131.64\n",
      "Iter 82  time=0.64  loss=7454.49  active=204585 feature_norm=131.52\n",
      "Iter 83  time=1.28  loss=7424.93  active=199164 feature_norm=132.28\n",
      "Iter 84  time=0.65  loss=7296.84  active=197888 feature_norm=132.30\n",
      "Iter 85  time=0.64  loss=7234.22  active=194933 feature_norm=132.53\n",
      "Iter 86  time=0.64  loss=7132.75  active=188414 feature_norm=132.30\n",
      "Iter 87  time=0.64  loss=7050.98  active=185573 feature_norm=132.06\n",
      "Iter 88  time=0.64  loss=6979.76  active=184271 feature_norm=131.56\n",
      "Iter 89  time=0.64  loss=6890.09  active=180638 feature_norm=130.76\n",
      "Iter 90  time=1.27  loss=6862.20  active=179394 feature_norm=130.26\n",
      "Iter 91  time=0.64  loss=6797.71  active=177698 feature_norm=130.06\n",
      "Iter 92  time=0.64  loss=6748.07  active=174919 feature_norm=129.90\n",
      "Iter 93  time=2.53  loss=6723.43  active=173513 feature_norm=129.95\n",
      "Iter 94  time=0.66  loss=6658.41  active=169698 feature_norm=129.83\n",
      "Iter 95  time=0.67  loss=6621.14  active=167218 feature_norm=129.64\n",
      "Iter 96  time=0.68  loss=6573.91  active=166354 feature_norm=129.53\n",
      "Iter 97  time=0.67  loss=6531.87  active=164428 feature_norm=129.25\n",
      "Iter 98  time=0.66  loss=6484.39  active=161317 feature_norm=128.97\n",
      "Iter 99  time=0.65  loss=6453.98  active=159238 feature_norm=128.64\n",
      "Iter 100 time=0.65  loss=6416.34  active=158444 feature_norm=128.56\n",
      "Iter 101 time=0.65  loss=6384.44  active=156646 feature_norm=128.29\n",
      "Iter 102 time=1.29  loss=6363.38  active=153675 feature_norm=128.18\n",
      "Iter 103 time=0.64  loss=6324.57  active=152979 feature_norm=127.95\n",
      "Iter 104 time=0.65  loss=6310.36  active=152798 feature_norm=127.83\n",
      "Iter 105 time=0.65  loss=6288.45  active=152178 feature_norm=127.71\n",
      "Iter 106 time=0.65  loss=6259.23  active=150269 feature_norm=127.50\n",
      "Iter 107 time=1.30  loss=6238.29  active=147443 feature_norm=127.10\n",
      "Iter 108 time=1.29  loss=6233.16  active=146672 feature_norm=127.08\n",
      "Iter 109 time=0.65  loss=6198.95  active=147228 feature_norm=126.95\n",
      "Iter 110 time=0.65  loss=6184.89  active=146768 feature_norm=126.88\n",
      "Iter 111 time=0.65  loss=6157.66  active=143756 feature_norm=126.70\n",
      "Iter 112 time=1.89  loss=6150.49  active=144007 feature_norm=126.57\n",
      "Iter 113 time=0.64  loss=6132.54  active=142730 feature_norm=126.49\n",
      "Iter 114 time=0.64  loss=6113.99  active=140782 feature_norm=126.22\n",
      "Iter 115 time=0.64  loss=6094.97  active=138894 feature_norm=126.05\n",
      "Iter 116 time=0.64  loss=6077.42  active=137177 feature_norm=125.74\n",
      "Iter 117 time=0.64  loss=6060.64  active=135895 feature_norm=125.62\n",
      "Iter 118 time=0.65  loss=6042.25  active=134059 feature_norm=125.28\n",
      "Iter 119 time=0.64  loss=6026.11  active=132837 feature_norm=125.03\n",
      "Iter 120 time=0.64  loss=6008.27  active=132059 feature_norm=124.72\n",
      "Iter 121 time=0.64  loss=5993.53  active=131416 feature_norm=124.55\n",
      "Iter 122 time=1.27  loss=5989.34  active=129510 feature_norm=124.17\n",
      "Iter 123 time=0.64  loss=5970.22  active=130157 feature_norm=124.28\n",
      "Iter 124 time=0.64  loss=5962.42  active=129694 feature_norm=124.12\n",
      "Iter 125 time=0.64  loss=5951.53  active=129037 feature_norm=123.97\n",
      "Iter 126 time=0.66  loss=5942.43  active=126488 feature_norm=123.44\n",
      "Iter 127 time=0.64  loss=5924.88  active=127410 feature_norm=123.48\n",
      "Iter 128 time=0.64  loss=5918.56  active=127454 feature_norm=123.41\n",
      "Iter 129 time=0.65  loss=5903.80  active=126126 feature_norm=123.21\n",
      "Iter 130 time=0.64  loss=5895.29  active=125493 feature_norm=123.07\n",
      "Iter 131 time=0.64  loss=5885.37  active=125497 feature_norm=122.96\n",
      "Iter 132 time=0.67  loss=5874.75  active=123790 feature_norm=122.70\n",
      "Iter 133 time=1.91  loss=5867.25  active=124921 feature_norm=122.70\n",
      "Iter 134 time=0.65  loss=5860.65  active=124210 feature_norm=122.61\n",
      "Iter 135 time=0.66  loss=5854.05  active=123181 feature_norm=122.58\n",
      "Iter 136 time=1.28  loss=5847.85  active=122673 feature_norm=122.45\n",
      "Iter 137 time=0.64  loss=5846.69  active=122016 feature_norm=122.41\n",
      "Iter 138 time=0.64  loss=5836.21  active=122044 feature_norm=122.31\n",
      "Iter 139 time=0.64  loss=5831.15  active=121960 feature_norm=122.30\n",
      "Iter 140 time=0.65  loss=5827.13  active=121015 feature_norm=122.19\n",
      "Iter 141 time=0.64  loss=5821.17  active=120872 feature_norm=122.24\n",
      "Iter 142 time=0.64  loss=5817.57  active=120810 feature_norm=122.18\n",
      "Iter 143 time=0.64  loss=5813.39  active=120701 feature_norm=122.17\n",
      "Iter 144 time=0.64  loss=5807.54  active=120184 feature_norm=122.08\n",
      "Iter 145 time=0.64  loss=5801.06  active=119172 feature_norm=122.01\n",
      "Iter 146 time=0.64  loss=5793.59  active=119307 feature_norm=121.95\n",
      "Iter 147 time=0.64  loss=5788.27  active=119292 feature_norm=121.91\n",
      "Iter 148 time=1.28  loss=5782.75  active=118120 feature_norm=121.80\n",
      "Iter 149 time=0.64  loss=5776.07  active=118026 feature_norm=121.74\n",
      "Iter 150 time=0.63  loss=5770.62  active=118379 feature_norm=121.69\n",
      "L-BFGS terminated with the maximum number of iterations\n",
      "Total seconds required for training: 126.501\n",
      "\n",
      "Storing the model\n",
      "Number of active features: 118379 (396086)\n",
      "Number of active attributes: 56341 (231345)\n",
      "Number of active labels: 13 (13)\n",
      "Writing labels\n",
      "Writing attributes\n",
      "Writing feature references for transitions\n",
      "Writing feature references for attributes\n",
      "Seconds required: 0.066\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crf = crfsuite.CRF(\n",
    "    verbose='true',\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=150,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "\n",
    "try:\n",
    "    crf.fit(X_train, y_train)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-CORP',\n",
       " 'B-GRP',\n",
       " 'I-GRP',\n",
       " 'B-PER',\n",
       " 'I-PER',\n",
       " 'I-CORP',\n",
       " 'B-CW',\n",
       " 'I-CW',\n",
       " 'B-PROD',\n",
       " 'I-PROD',\n",
       " 'B-LOC',\n",
       " 'I-LOC']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = list(crf.classes_)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8075812824435331"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = crf.predict(X_dev)\n",
    "metrics.flat_f1_score(y_dev, y_pred,\n",
    "                      average='macro', labels=labels)\n",
    "\n",
    "# 0.814325281001763"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8009467990614277"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = crf.predict(X_test)\n",
    "metrics.flat_f1_score(y_test, y_pred,\n",
    "                      average='macro', labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-CORP', 'O', 'O']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[69]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-CORP', 'I-CORP', 'O']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[69]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
