{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26999,"status":"ok","timestamp":1683131268165,"user":{"displayName":"NILOY FARHAN","userId":"17480483962253324951"},"user_tz":-360},"id":"7yMmUdwp3JaS","outputId":"5fb6a350-0c8c-4e40-96d7-8f8def0f91da"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://github.com/csebuetnlp/normalizer\n","  Cloning https://github.com/csebuetnlp/normalizer to /tmp/pip-req-build-sg7krgi3\n","  Running command git clone --filter=blob:none --quiet https://github.com/csebuetnlp/normalizer /tmp/pip-req-build-sg7krgi3\n","  Resolved https://github.com/csebuetnlp/normalizer to commit d80c3c484e1b80268f2b2dfaf7557fe65e34f321\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from normalizer==0.0.1) (2022.10.31)\n","Requirement already satisfied: emoji==1.4.2 in /usr/local/lib/python3.10/dist-packages (from normalizer==0.0.1) (1.4.2)\n","Requirement already satisfied: ftfy==6.0.3 in /usr/local/lib/python3.10/dist-packages (from normalizer==0.0.1) (6.0.3)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy==6.0.3->normalizer==0.0.1) (0.2.6)\n"]}],"source":["!pip install tokenizers -q\n","!pip install git+https://github.com/csebuetnlp/normalizer\n","!pip install transformers -q"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":5199,"status":"ok","timestamp":1683131273360,"user":{"displayName":"NILOY FARHAN","userId":"17480483962253324951"},"user_tz":-360},"id":"toxbp60C3KAs"},"outputs":[],"source":["from normalizer import normalize\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.preprocessing import OneHotEncoder\n","import pickle\n","import torch\n","from transformers import ElectraTokenizer"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1683131273360,"user":{"displayName":"NILOY FARHAN","userId":"17480483962253324951"},"user_tz":-360},"id":"wPMnsPdW3bpC"},"outputs":[],"source":["\n","model_name = '/content/drive/MyDrive/Thesis/BERTOUTPUT/checkpoint-11000/'\n","tokenizer = ElectraTokenizer.from_pretrained(model_name)"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1683131273361,"user":{"displayName":"NILOY FARHAN","userId":"17480483962253324951"},"user_tz":-360},"id":"8YYmonYL3Yuf","outputId":"8db442ac-6547-42e9-c0e7-cb0f9826c5c9"},"outputs":[{"output_type":"stream","name":"stdout","text":["[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] J\n","[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] o\n","[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] h\n","[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] n\n","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n","[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0] ট\n","[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0] ো\n","[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0] ক\n","[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0] ি\n","[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0] ও\n","[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]  \n","[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0] ব\n","[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0] ি\n","[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0] শ\n","[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0] ্\n","[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0] ব\n","[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0] ব\n","[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0] ি\n","[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0] দ\n","[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0] ্\n","[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0] য\n","[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0] া\n","[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0] ল\n","[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0] য\n","[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0] ়\n","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] এ\n","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] প\n","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] ড়\n","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] ে\n","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] a\n","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] n\n","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] d\n","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] o\n","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] w\n","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] n\n","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] s\n","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] a\n","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] n\n","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0] i\n","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1] P\n","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1] h\n","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1] o\n","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1] n\n","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1] e\n","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]  \n","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1] 1\n","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1] 4\n","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] m\n","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] a\n","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] d\n","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] e\n","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] b\n","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] y\n","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0] A\n","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1] p\n","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1] p\n","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1] l\n","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1] e\n","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] ,\n","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] n\n","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] o\n","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] t\n","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] M\n","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] i\n","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] c\n","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] r\n","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] o\n","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] s\n","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] o\n","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] f\n","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] t\n","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] .\n","John [0 1 0 0 0 0 0 0 0 0 0 0 0]\n","টোকিও [0 0 0 0 0 0 0 1 0 0 0 0 0]\n","বিশ্ববিদ্যালয় [0 0 0 0 0 0 0 0 1 0 0 0 0]\n","এ [1 0 0 0 0 0 0 0 0 0 0 0 0]\n","পড়ে [1 0 0 0 0 0 0 0 0 0 0 0 0]\n","and [1 0 0 0 0 0 0 0 0 0 0 0 0]\n","owns [1 0 0 0 0 0 0 0 0 0 0 0 0]\n","an [1 0 0 0 0 0 0 0 0 0 0 0 0]\n","iPhone [0 0 0 0 0 0 0 0 0 0 0 1 0]\n","14 [0 0 0 0 0 0 0 0 0 0 0 0 1]\n","made [1 0 0 0 0 0 0 0 0 0 0 0 0]\n","by [1 0 0 0 0 0 0 0 0 0 0 0 0]\n","Apple, [0 0 0 0 0 0 0 0 0 0 0 1 0]\n","not [1 0 0 0 0 0 0 0 0 0 0 0 0]\n","Microsoft. [1 0 0 0 0 0 0 0 0 0 0 0 0]\n"]}],"source":["class TrieNode:\n","    def __init__(self):\n","        self.children = {}\n","        self.is_end_of_word = False\n","\n","class Trie:\n","    def __init__(self):\n","        self.root = TrieNode()\n","        self.entity_tags = [\"PER\", \"LOC\", \"CW\", \"CORP\", \"GRP\", \"PROD\"]\n","        self.tag_encoding = {\"O\": 0, \"B-PER\": 1, \"I-PER\": 2, \"B-LOC\": 3, \"I-LOC\": 4,\n","                             \"B-CW\": 5, \"I-CW\": 6, \"B-CORP\": 7, \"I-CORP\": 8, \n","                             \"B-GRP\": 9, \"I-GRP\": 10, \"B-PROD\": 11, \"I-PROD\": 12}\n","\n","    def insert(self, word, entity_type):\n","        node = self.root\n","        for char in word:\n","            if char not in node.children:\n","                node.children[char] = TrieNode()\n","            node = node.children[char]\n","        node.is_end_of_word = True\n","        node.entity_type = entity_type\n","\n","    def search(self, sentence):\n","        encoding = [0] * len(sentence)\n","        for i in range(len(sentence)):\n","            node = self.root\n","            for j in range(i, len(sentence)):\n","                char = sentence[j]\n","                if char not in node.children:\n","                    break\n","                node = node.children[char]\n","                if node.is_end_of_word:\n","                    entity_type = node.entity_type\n","                    entity_length = j - i + 1\n","                    start_index = i\n","                    end_index = j\n","                    for k in range(start_index, end_index + 1):\n","                        if k == start_index:\n","                            encoding[k] = self.tag_encoding[\"B-\" + entity_type]\n","                        else:\n","                            encoding[k] = self.tag_encoding[\"I-\" + entity_type]\n","                    break\n","        one_hot_encoding = [[0] * 13 for i in range(len(encoding))]\n","        for i, tag in enumerate(encoding):\n","            one_hot_encoding[i][tag] = 1\n","        return one_hot_encoding\n","\n","import numpy as np\n","\n","def group_encodings_by_word(encoding, sentence):\n","    # Create an empty list to store the word encodings\n","    word_encodings = []\n","    \n","    # Create an empty list to store the current word encoding\n","    current_word_encoding = []\n","    \n","    # Create an empty string to store the current word\n","    current_word = \"\"\n","    \n","    # Iterate over each character encoding and character in the input encoding list and sentence, respectively\n","    for char_encoding, char in zip(encoding, sentence):\n","        # If the current character is a whitespace character, finish the current word and add its first character encoding to the word encodings list\n","        if char == \" \":\n","            if len(current_word_encoding) > 0:\n","                word_encodings.append(np.array(current_word_encoding[0]))\n","                current_word_encoding = []\n","            current_word = \"\"\n","        # If the current character is part of a word, append the character encoding to the current word encoding and the character to the current word\n","        else:\n","            current_word_encoding.append(char_encoding)\n","            current_word += char\n","    \n","    # Add the last word encoding to the word encodings list, if it exists\n","    if len(current_word_encoding) > 0:\n","        word_encodings.append(np.array(current_word_encoding[0]))\n","    \n","    # Return the word encodings as a NumPy array\n","    return np.array(word_encodings)\n","\n","import pickle\n","\n","def save_trie(trie, filename):\n","    with open(filename, \"wb\") as f:\n","        pickle.dump(trie, f)\n","\n","def load_trie(filename):\n","    with open(filename, \"rb\") as f:\n","        trie = pickle.load(f)\n","    return trie\n","\n","if __name__ == \"__main__\":\n","    trie = Trie()\n","\n","    # Insert some named entities into the Trie\n","    trie.insert(\"John\", \"PER\")\n","    trie.insert(\"New York\", \"LOC\")\n","    trie.insert(\"iPhone 14\", \"PROD\")\n","    trie.insert(\"টোকিও বিশ্ববিদ্যালয়\", \"CORP\")\n","    trie.insert(\"Apple\", \"CORP\")\n","    trie.insert(\"Apple\", \"PROD\")\n","    # Search for named entities in a sentence\n","    sentence = \"John টোকিও বিশ্ববিদ্যালয় এ পড়ে and owns an iPhone 14 made by Apple, not Microsoft.\"\n","    encoding = trie.search(sentence)\n","    for i, word in enumerate(sentence):\n","        print(encoding[i],word)\n","    \n","    word_encodings = group_encodings_by_word(encoding,sentence)\n","    \n","# Print the word encodings\n","    for word, word_encoding in zip(sentence.split(), word_encodings):\n","        print(word, word_encoding)"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":8325,"status":"ok","timestamp":1683131970115,"user":{"displayName":"NILOY FARHAN","userId":"17480483962253324951"},"user_tz":-360},"id":"Lzi6x6M_4BR5"},"outputs":[],"source":["trie = load_trie('/content/drive/MyDrive/Bracu/THESIS/Trie DS/Trie old.bin')\n"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":721,"status":"ok","timestamp":1683131295072,"user":{"displayName":"NILOY FARHAN","userId":"17480483962253324951"},"user_tz":-360},"id":"85oUKuLv3bJK"},"outputs":[],"source":["\n","def Gtoken(text):\n","  inputs = tokenizer.encode_plus(text, return_tensors='pt')\n","\n","  encoded_dict = tokenizer.encode_plus(\n","                  text,       # Sentence to encode.\n","                  add_special_tokens = False, # Add '[CLS]' and '[SEP]'\n","                  max_length = 314,           # Pad & truncate all sentences.\n","                  padding = 'max_length',\n","                  return_attention_mask = True,   # Construct attn. masks.\n","                  return_tensors = 'pt',\n","                  truncation=False)\n","  input_ids = encoded_dict['input_ids']\n","  tokenized = tokenizer.convert_ids_to_tokens([i.item() for i in input_ids.squeeze() if i > 1])\n","  return \"< \"+\" \".join(tokenized)+\" >\""]},{"cell_type":"code","source":["%cd /content"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7HhjOZaafkga","executionInfo":{"status":"ok","timestamp":1683132153738,"user_tz":-360,"elapsed":8,"user":{"displayName":"NILOY FARHAN","userId":"17480483962253324951"}},"outputId":"0a86ea02-5ae4-412a-9b3e-19609d4df90d"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":304210,"status":"ok","timestamp":1683132460642,"user":{"displayName":"NILOY FARHAN","userId":"17480483962253324951"},"user_tz":-360},"id":"f8XIP7fPWnUt","outputId":"b084cc4b-916d-4839-b419-31f83fa541df"},"outputs":[{"output_type":"stream","name":"stdout","text":["PER\n","LOC\n","CW\n","CORP\n","GRP\n","PROD\n"]}],"source":["entity_tags = [\"PER\", \"LOC\", \"CW\", \"CORP\", \"GRP\", \"PROD\"]\n","for entity in entity_tags:\n","    with open('{}.txt'.format(entity), 'r', encoding = 'utf-8') as file:\n","        print(entity)\n","        for line in file:\n","            trie.insert(Gtoken(normalize(line.strip())), entity)\n","\n","# save_trie(tree, \"Trie.bin\")"]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/Bracu/THESIS/Trie DS/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CLb-2b-FcJE4","executionInfo":{"status":"ok","timestamp":1683132832597,"user_tz":-360,"elapsed":3,"user":{"displayName":"NILOY FARHAN","userId":"17480483962253324951"}},"outputId":"74cfe26a-9ea2-48db-ae80-794739363912"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Bracu/THESIS/Trie DS\n"]}]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":269598,"status":"ok","timestamp":1683131762851,"user":{"displayName":"NILOY FARHAN","userId":"17480483962253324951"},"user_tz":-360},"id":"9jV-tDmw3wD-","colab":{"base_uri":"https://localhost:8080/"},"outputId":"05c4e614-1981-4406-b3b6-69ca366dabcc"},"outputs":[{"output_type":"stream","name":"stdout","text":["PER\n","PER\n","LOC\n","LOC\n","CW\n","CW\n","CORP\n","CORP\n","GRP\n","GRP\n","PROD\n","PROD\n"]}],"source":["\n","tree = Trie()\n","entity_tags = [\"PER\", \"LOC\", \"CW\", \"CORP\", \"GRP\", \"PROD\"]\n","for entity in entity_tags:\n","    with open('KE/{}/{}.txt'.format(entity,entity), 'r', encoding = 'utf-8') as file:\n","        print(entity)\n","        for line in file:\n","            tree.insert(Gtoken(normalize(line.strip())), entity)\n","    with open('KE/{}.txt'.format(entity), 'r', encoding = 'utf-8') as file:\n","        print(entity)\n","        for line in file:\n","            tree.insert(Gtoken(normalize(line.strip())), entity)\n","\n","save_trie(tree, \"/content/Trie.bin\")\n"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cezUEVTz4bVU","executionInfo":{"status":"ok","timestamp":1683132774247,"user_tz":-360,"elapsed":407,"user":{"displayName":"NILOY FARHAN","userId":"17480483962253324951"}},"outputId":"922e54fe-b680-4e6d-baa2-33c15bb02383"},"outputs":[{"output_type":"stream","name":"stdout","text":["< [1 0 0 0 0 0 0 0 0 0 0 0 0]\n","আন্দ্রে [0 0 0 0 0 1 0 0 0 0 0 0 0]\n","##য়া [1 0 0 0 0 0 0 0 0 0 0 0 0]\n","নাভ [1 0 0 0 0 0 0 0 0 0 0 0 0]\n","##াগের [1 0 0 0 0 0 0 0 0 0 0 0 0]\n","##ো [1 0 0 0 0 0 0 0 0 0 0 0 0]\n","এবং [1 0 0 0 0 0 0 0 0 0 0 0 0]\n","অ্যা [1 0 0 0 0 0 0 0 0 0 0 0 0]\n","##গো [1 0 0 0 0 0 0 0 0 0 0 0 0]\n","##স্টি [1 0 0 0 0 0 0 0 0 0 0 0 0]\n","##নো [1 0 0 0 0 0 0 0 0 0 0 0 0]\n","বেজ [1 0 0 0 0 0 0 0 0 0 0 0 0]\n","##ানোর [1 0 0 0 0 0 0 0 0 0 0 0 0]\n","প্রতিকৃতি [0 0 0 0 0 1 0 0 0 0 0 0 0]\n","বাড়ীর [1 0 0 0 0 0 0 0 0 0 0 0 0]\n","ছোট [0 0 0 0 0 1 0 0 0 0 0 0 0]\n","বউ [0 0 0 0 0 1 0 0 0 0 0 0 0]\n","অ্যাম [0 0 0 0 0 1 0 0 0 0 0 0 0]\n","##িটি [0 0 0 0 0 0 1 0 0 0 0 0 0]\n","ইনস্টিটিউট [0 0 0 0 0 1 0 0 0 0 0 0 0]\n","অফ [1 0 0 0 0 0 0 0 0 0 0 0 0]\n","বায়ো [1 0 0 0 0 0 0 0 0 0 0 0 0]\n","##টেক [1 0 0 0 0 0 0 0 0 0 0 0 0]\n","##নো [1 0 0 0 0 0 0 0 0 0 0 0 0]\n","##লজি [1 0 0 0 0 0 0 0 0 0 0 0 0]\n","> [1 0 0 0 0 0 0 0 0 0 0 0 0]\n","(26, 13)\n"]}],"source":["sentence = Gtoken(normalize(\"আন্দ্রেয়া নাভাগেরো এবং অ্যাগোস্টিনো বেজানোর প্রতিকৃতি বাড়ীর ছোট বউ অ্যামিটি ইনস্টিটিউট অফ বায়োটেকনোলজি\"))\n","encoding = trie.search(sentence)\n","trie.insert(Gtoken(normalize('কুত্তা')), 'PER')\n","# for i, word in enumerate(sentence):\n","#     # print(encoding[i],word)\n","\n","word_encodings = group_encodings_by_word(encoding,sentence)\n","\n","# Print the word encodings\n","for word, word_encoding in zip(sentence.split(), word_encodings):\n","    print(word, word_encoding)\n","\n","print(word_encodings.shape)"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"_sapBdDs4v0g","executionInfo":{"status":"ok","timestamp":1683132861258,"user_tz":-360,"elapsed":371,"user":{"displayName":"NILOY FARHAN","userId":"17480483962253324951"}}},"outputs":[],"source":["def Gazetteer(sentence, pad = True):\n","  sentence = Gtoken(normalize(sentence))\n","  encoding = trie.search(sentence)\n","  tensor = group_encodings_by_word(encoding,sentence)\n","  if pad:\n","    current_size = tensor.size(0)\n","    if current_size >= 64:\n","        return tensor[:64, :]  # if the tensor is larger than (64, 13), truncate it\n","    \n","    padded_tensor = torch.full((64, 13),-100.)\n","    padded_tensor[:current_size, :] = tensor  # copy the input tensor to the padded tensor\n","    return padded_tensor\n","  else: \n","    return tensor"]},{"cell_type":"code","source":["tensor"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":165},"id":"jwHvDQw1iI_v","executionInfo":{"status":"error","timestamp":1683132928520,"user_tz":-360,"elapsed":966,"user":{"displayName":"NILOY FARHAN","userId":"17480483962253324951"}},"outputId":"4a3a9344-6020-403c-cc1b-e319c5183c04"},"execution_count":39,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-39-73f982c7f466>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'tensor' is not defined"]}]},{"cell_type":"code","source":["save_trie(trie, \"Trie final.bin\")"],"metadata":{"id":"IUceRdRPiHMX","executionInfo":{"status":"ok","timestamp":1683132852757,"user_tz":-360,"elapsed":9330,"user":{"displayName":"NILOY FARHAN","userId":"17480483962253324951"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["Gazetteer(\"ব্লেকলকের চিত্রকর্মগুলি ২০২০ চলচ্চিত্র আমি বিষয় শেষ করার চিন্তা করছি এর একটি মূল প্লট পয়েন্ট\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":287},"id":"IZw6Oj_ViJpZ","executionInfo":{"status":"error","timestamp":1683132893131,"user_tz":-360,"elapsed":8,"user":{"displayName":"NILOY FARHAN","userId":"17480483962253324951"}},"outputId":"bd4f4ea1-0356-4b06-ec24-a8706c72976e"},"execution_count":37,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-37-1ab8186765bd>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mGazetteer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ব্লেকলকের চিত্রকর্মগুলি ২০২০ চলচ্চিত্র আমি বিষয় শেষ করার চিন্তা করছি এর একটি মূল প্লট পয়েন্ট\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-35-da0a2b8646ce>\u001b[0m in \u001b[0;36mGazetteer\u001b[0;34m(sentence, pad)\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup_encodings_by_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mcurrent_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcurrent_size\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# if the tensor is larger than (64, 13), truncate it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"]}]},{"cell_type":"code","source":[],"metadata":{"id":"DR-RtAAkiXep"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"mount_file_id":"15tgD1jVir3apRCz3WrZ7oAk9H_gPSomM","authorship_tag":"ABX9TyO3VnEsTSGNwRSK+OeLPgZ7"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}